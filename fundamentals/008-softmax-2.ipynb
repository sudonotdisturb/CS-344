{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `008-softmax-2`\n",
    "\n",
    "Task: more practice using the `softmax` function, and connect it with the `sigmoid` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this example: `x = tensor([0.1, 0.2, 0.3])`\n",
    "\n",
    "1. If `p=softmax(x)`, what is `p.sum()`? Can you get `p.sum()` to change by changing `x`? Can you make `p.min()` be less than 0?\n",
    "2. Try making an `xx` that's the same as `x` except that `xx[2] = 100`, and let `p = softmax(xx)` again. How does `p[2]` compare with `p[0]` and `p[1]`?\n",
    "3. Try `torch.sigmoid(tensor(0.1))`. Can you write an expression that uses `torch.softmax` to get the same output?\n",
    "\n",
    "**Hint for \\#3**: Give `sigmoid` a two-element `tensor`. One of those elements will be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Add code and Markdown cells for each of the listed tasks above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tensor([0.1, 0.2, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.softmax(x, dim=0)\n",
    "p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(1.))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to get p.sum() to change by changing x\n",
    "p1 = torch.softmax(x + 100, dim=0)\n",
    "p2 = torch.softmax(x * 3, dim=0)\n",
    "p1.sum(), p2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3006), tensor(0.3006), tensor(0.2397))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.min(), p1.min(), p2.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3333), tensor(0.))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt to get p.min() to be less than 0\n",
    "p3 = torch.softmax(x * 0.00001, dim=0)\n",
    "p4 = torch.softmax(x * 100000, dim=0)\n",
    "p3.min(), p4.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on my (limited) testing, `p.sum()` cannot change by changing `x`; `p.sum()` will always equal 1. `p.min()` also cannot be less than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0638e-44, 4.4842e-44, 1.0000e+00])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = tensor([0.1, 0.2, 100])\n",
    "p = torch.softmax(xx, dim=0)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`p[2]` is much, much greater than `p[0]` and `p[1]`, to the point where `p[2]` is pretty much 1 and `p[0]` and `p[1]` are extremely tiny (almost insignificant) integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5250, 0.5000]), tensor([0.5250, 0.4750]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = torch.sigmoid(tensor([0.1, 0]))\n",
    "soft = torch.softmax(tensor([0.1, 0]), dim=0)\n",
    "sig, soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5250), tensor(0.5250))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig[0], soft[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A valid probability distribution has no negative numbers and sums to 1. Is `softmax(x)` a valid probability distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on my observations, yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sometimes `x` is called the \"logits\" and `x.softmax().log()` (or `x.log_softmax()`) is called the \"logprobs\", short for \"log probabilities\". Compute the logits, logprobs, and probabilities for `x` in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3006, 0.3322, 0.3672]),\n",
       " tensor([4.0638e-44, 4.4842e-44, 1.0000e+00]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = x\n",
    "logprobs = logits.softmax(dim=0).log()\n",
    "probs = logprobs.exp()\n",
    "\n",
    "# Compare the computed probabilities with the results of softmax() earlier\n",
    "probs, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In light of your observations about `xx[2]` and `p[2]` above, why might `softmax` be an appropriate name for this function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`softmax` might be an appropriate name because it maps large numbers to a value close to 1 and small numbers to a value closer to 0, relative to all of the numbers passed to `softmax`. As with `xx[2]` and `p[2]`, `softmax` mapped 100 to a 1, indicating that not only was 100 the largest value in the given tensor, but that it was much greater than the other values in the same tensor. If a 1 was used instead of 100, `softmax` would have mapped each value in the tensor to a less sparse range (see below). Therefore, the \"soft\" in `softmax` refers to the max number in the given tensor being mapped to a relatively larger value than what the others are mapped to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.0638e-44, 4.4842e-44, 1.0000e+00]),\n",
       " tensor([0.2191, 0.2421, 0.5388]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_100 = tensor([0.1, 0.2, 100])\n",
    "p_100 = torch.softmax(x_100, dim=0)\n",
    "\n",
    "x_1 = tensor([0.1, 0.2, 1])\n",
    "p_1 = torch.softmax(x_1, dim=0)\n",
    "\n",
    "p_100, p_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
