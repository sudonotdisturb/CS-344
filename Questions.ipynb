{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does combining all image transformations into one transformation work when presizing? And what does image interpolation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still don't get how \"freezing\" and \"unfreezing\" and `fit_one_cycle` works. Could you explain more about why we use them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we pass in `slice(1e-6, 1e-4)` to specify the learning rates when 1e-6 is the earliest layer? Shouldn't the earliest layer have the highest learning rate, and then we fine tune by decreasing the learning rate from there?\n",
    "\n",
    "    We want to choose a very low initial learning rate so that the learning rate will not be too big to handle. If the learning rate is too big starting off, then the model will be highly inaccurate. Starting small and working up to a higher learning rate is important to finding the rate that is \"just right.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does storing all text as lowercase with the 'xxmaj' special token save compute and memory resources? Doesn't storing 'xxmaj' require more memory when there are more capitalized words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get the shapes of `x` and `y` from the `LMDataLoader`, we get a matrix of size 64x72. What does this shape mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a language model make predictions?\n",
    "\n",
    "    The model starts off with a xxbos token. It makes a prediction on what token comes next. This prediction is fed back into the model, so now the model takes into account the first two tokens. This helps it predict the third token.\n",
    "\n",
    "    A language model gives two things: a prediction for the next word, and an array of how likely each word is in the vocab to be the next word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
