{"nbformat":4,"nbformat_minor":0,"metadata":{"jupytext":{"split_at_heading":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"10_nlp.ipynb","provenance":[{"file_id":"https://github.com/Calvin-CS344-21SP/ai-portfolio-sudonotdisturb/blob/main/reading/10_nlp.ipynb","timestamp":1617999802383}],"collapsed_sections":["FkRkKNR2TZ1k"],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LDJUTi_uTfv-","executionInfo":{"status":"ok","timestamp":1618175063518,"user_tz":240,"elapsed":3117,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"a771dd71-db39-4891-d87c-d4cde73449c5"},"source":["!pip install -U fastbook torchtext==0.8.1"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: fastbook in /usr/local/lib/python3.7/dist-packages (0.0.16)\n","Requirement already up-to-date: torchtext==0.8.1 in /usr/local/lib/python3.7/dist-packages (0.8.1)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from fastbook) (2.23.0)\n","Requirement already satisfied, skipping upgrade: fastai>=2.1 in /usr/local/lib/python3.7/dist-packages (from fastbook) (2.3.0)\n","Requirement already satisfied, skipping upgrade: pip in /usr/local/lib/python3.7/dist-packages (from fastbook) (19.3.1)\n","Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from fastbook) (1.1.5)\n","Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.7/dist-packages (from fastbook) (0.1.95)\n","Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from fastbook) (20.9)\n","Requirement already satisfied, skipping upgrade: ipywidgets in /usr/local/lib/python3.7/dist-packages (from fastbook) (7.6.3)\n","Requirement already satisfied, skipping upgrade: nbdev>=0.2.38 in /usr/local/lib/python3.7/dist-packages (from fastbook) (1.1.14)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (4.41.1)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (1.19.5)\n","Requirement already satisfied, skipping upgrade: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (1.7.1)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastbook) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastbook) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastbook) (1.24.3)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastbook) (2.10)\n","Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai>=2.1->fastbook) (0.22.2.post1)\n","Requirement already satisfied, skipping upgrade: torchvision<0.9,>=0.8 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.1->fastbook) (0.8.2)\n","Requirement already satisfied, skipping upgrade: spacy<3 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.1->fastbook) (2.2.4)\n","Requirement already satisfied, skipping upgrade: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.1->fastbook) (1.0.0)\n","Requirement already satisfied, skipping upgrade: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.1->fastbook) (7.1.2)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from fastai>=2.1->fastbook) (1.4.1)\n","Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai>=2.1->fastbook) (3.2.2)\n","Requirement already satisfied, skipping upgrade: fastcore<1.4,>=1.3.8 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.1->fastbook) (1.3.19)\n","Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai>=2.1->fastbook) (3.13)\n","Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastbook) (2018.9)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastbook) (2.8.1)\n","Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastbook) (2.4.7)\n","Requirement already satisfied, skipping upgrade: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->fastbook) (5.1.2)\n","Requirement already satisfied, skipping upgrade: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->fastbook) (5.5.0)\n","Requirement already satisfied, skipping upgrade: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->fastbook) (4.10.1)\n","Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->fastbook) (3.5.1)\n","Requirement already satisfied, skipping upgrade: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->fastbook) (5.0.5)\n","Requirement already satisfied, skipping upgrade: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->fastbook) (1.0.0)\n","Requirement already satisfied, skipping upgrade: nbconvert<6 in /usr/local/lib/python3.7/dist-packages (from nbdev>=0.2.38->fastbook) (5.6.1)\n","Requirement already satisfied, skipping upgrade: fastrelease in /usr/local/lib/python3.7/dist-packages (from nbdev>=0.2.38->fastbook) (0.1.11)\n","Requirement already satisfied, skipping upgrade: jupyter-client<=6.1.12 in /usr/local/lib/python3.7/dist-packages (from nbdev>=0.2.38->fastbook) (5.3.5)\n","Requirement already satisfied, skipping upgrade: jupyter in /usr/local/lib/python3.7/dist-packages (from nbdev>=0.2.38->fastbook) (1.0.0)\n","Requirement already satisfied, skipping upgrade: ghapi in /usr/local/lib/python3.7/dist-packages (from nbdev>=0.2.38->fastbook) (0.1.16)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->torchtext==0.8.1) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.1->fastbook) (1.0.1)\n","Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai>=2.1->fastbook) (2.0.5)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai>=2.1->fastbook) (54.2.0)\n","Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai>=2.1->fastbook) (1.0.5)\n","Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai>=2.1->fastbook) (1.0.0)\n","Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai>=2.1->fastbook) (0.4.1)\n","Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai>=2.1->fastbook) (0.8.2)\n","Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai>=2.1->fastbook) (1.1.3)\n","Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai>=2.1->fastbook) (3.0.5)\n","Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai>=2.1->fastbook) (7.4.0)\n","Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai>=2.1->fastbook) (1.0.5)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.1->fastbook) (0.10.0)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.1->fastbook) (1.3.1)\n","Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->fastbook) (1.15.0)\n","Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->fastbook) (0.2.0)\n","Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->fastbook) (2.6.0)\n","Requirement already satisfied, skipping upgrade: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->fastbook) (4.7.1)\n","Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->fastbook) (4.4.2)\n","Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->fastbook) (4.8.0)\n","Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->fastbook) (0.8.1)\n","Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->fastbook) (2.6.1)\n","Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->fastbook) (1.0.18)\n","Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->fastbook) (0.7.5)\n","Requirement already satisfied, skipping upgrade: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->fastbook) (5.1.1)\n","Requirement already satisfied, skipping upgrade: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets->fastbook) (5.3.1)\n","Requirement already satisfied, skipping upgrade: jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook) (2.11.3)\n","Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook) (0.8.4)\n","Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook) (0.3)\n","Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook) (3.3.0)\n","Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook) (0.4.4)\n","Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook) (0.7.1)\n","Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6->nbdev>=0.2.38->fastbook) (1.4.3)\n","Requirement already satisfied, skipping upgrade: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<=6.1.12->nbdev>=0.2.38->fastbook) (22.0.3)\n","Requirement already satisfied, skipping upgrade: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->nbdev>=0.2.38->fastbook) (5.2.0)\n","Requirement already satisfied, skipping upgrade: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->nbdev>=0.2.38->fastbook) (5.0.3)\n","Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3->fastai>=2.1->fastbook) (3.8.1)\n","Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->fastbook) (0.7.0)\n","Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->fastbook) (0.2.5)\n","Requirement already satisfied, skipping upgrade: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->fastbook) (1.5.0)\n","Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->fastbook) (0.9.3)\n","Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.4->nbconvert<6->nbdev>=0.2.38->fastbook) (1.1.1)\n","Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert<6->nbdev>=0.2.38->fastbook) (0.5.1)\n","Requirement already satisfied, skipping upgrade: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->nbdev>=0.2.38->fastbook) (1.9.0)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3->fastai>=2.1->fastbook) (3.4.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qrls7fPvTZ1L"},"source":["# NLP Deep Dive: RNNs\n","\n","*Self-supervised learning*: no need to give labels to the model, just feed it a ton of texts. The model automatically gets labels from the data.\n","\n","We can download a pretrained English language model, fine-tune it to predict words in a movie review, and then apply it to classification. Thus, there are 3 stages to this transfer learning in NLP:\n","\n","1. Download the pretrained language model, trained on Wikipedia articles.\n","2. Fine-tune this model on the IMDb reviews data.\n","3. Use this to classify reviews as positive or negative."]},{"cell_type":"markdown","metadata":{"id":"GNFMWvjETZ1N"},"source":["## Text Preprocessing\n","\n","Steps for processing text:\n","\n","1. Tokenization:: convert text into a list of tokens (words, subwords, characters, etc.)\n","2. Numericalization:: create a list of all unique words/tokens that appear (this is the vocab). Then, take the tokenized text and convert each token into a number (the index of that token in the vocab).\n","3. Create model data loader:: use fastai's `LMDataLoader` class to handle text data.\n","4. Create language model:: use a recurrent neural network (RNN) to handle input lists which could be arbitrarily large or small.\n","\n","The independent variable is the sequence of words from first to second-to-last, and the dependent variable is the sequence of words from second word to last.\n","\n","The vocab will consist of words already in the vocabulary of the pretrained model and new words specific to the corpus (documents).\n"]},{"cell_type":"markdown","metadata":{"id":"qvvfg0mTTZ1O"},"source":["### Tokenization\n","\n","Three approaches to tokenization:\n","\n","* Word-based: split a sentences into its individual words (split on the spaces). Use language-specific rules to separate parts of meaning even if there is no space (e.g. turning \"don't\" into \"do n't\"). Punctuation marks are usually split into individual tokens.\n","* Subword-based:: split words into smaller parts, based on the most commonly occurring substrings (e.g. \"occasion\" => \"o c ca sion\").\n","* Character-based: split a sentence into individual characters."]},{"cell_type":"markdown","metadata":{"id":"QJMNCzJNTZ1P"},"source":["### Word Tokenization with fastai"]},{"cell_type":"code","metadata":{"id":"cLHockOfTZ1P","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1618175809765,"user_tz":240,"elapsed":31981,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"1b07f953-d1a6-46d3-919a-e88aea30127a"},"source":["from fastai.text.all import *\n","path = untar_data(URLs.IMDB, force_download=True)"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"ebM0uzZUTZ1Q"},"source":["Grab all text files from `path`, using `folders` to restrict the search to just the provided subfolders."]},{"cell_type":"code","metadata":{"id":"LlVPsJ0_TZ1Q","executionInfo":{"status":"ok","timestamp":1618177102230,"user_tz":240,"elapsed":1156,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":["files = get_text_files(path, folders = ['train', 'test', 'unsup'])"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UZGOwZsmTZ1R"},"source":["Get the first 75 characters of the first file:"]},{"cell_type":"code","metadata":{"id":"nCYwoxnKTZ1R","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1618177103799,"user_tz":240,"elapsed":158,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"defa3e6d-a707-4807-b8d6-4473e61dff9a"},"source":["txt = files[0].open().read()\n","txt[:75]"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tenshu is imprisoned and sentenced to death. When he survives electrocution'"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"6bRE3bRETZ1S"},"source":["Default English word tokenizer for fastai uses a library called *spaCy*. We use `WordTokenizer` below, since that refers to fastai's current default word tokenizer (which may or may not be *spaCy*).\n","\n","We tokenize the file we got above, then print the first 30 tokens with `coll_repr`. Since fastai's tokenizers take a collection/list of documents to tokenize, `txt` is wrapped in a list."]},{"cell_type":"code","metadata":{"id":"yvEN1ts4TZ1S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618177105955,"user_tz":240,"elapsed":273,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"956e8809-2b83-4bb1-b210-f60fd50d8a3a"},"source":["spacy = WordTokenizer()\n","tokens = first(spacy([txt]))\n","print(coll_repr(tokens, 30))"],"execution_count":32,"outputs":[{"output_type":"stream","text":["(#260) ['Tenshu','is','imprisoned','and','sentenced','to','death','.','When','he','survives','electrocution','the','government','officials','give','him','a','choice','to','either','be','electrocute','at','a','greater','degree','or','agree','to'...]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Amh4eBNzTZ1T"},"source":["spaCy is smart about separating punctuation; it knows to separate \".\" when it ends a sentence and not to separate it when \".\" is contained within a word/abbreviation."]},{"cell_type":"code","metadata":{"id":"6zzXIWf3TZ1T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618177107231,"user_tz":240,"elapsed":163,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"07460a5f-1e10-4168-9d0c-7feb85430ef3"},"source":["first(spacy(['The U.S. dollar $1 is 1.00.']))"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#8) ['The','U.S.','dollar','$','1','is','1.00','.']"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"y88WXkYKTZ1T"},"source":["Use fastai's `Tokenizer` class to add *special tokens* to the token list. These tokens start with 'xx'. The main special tokens are:\n","\n","* xxbos:: Indicates the beginning of a stream/text (BOS = \"beginning of stream\")\n","* xxmaj:: Indicates the next word begins with a capital (since we lowercased everything)\n","* xxunk:: Indicates the next word is unknown\n","\n","These special tokens are used to make it easier for the model to store and process the text data (e.g. with xxmaj, every word can be lowercase, saving compute and memory resources)."]},{"cell_type":"code","metadata":{"id":"3KJZI4JfTZ1U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618177108068,"user_tz":240,"elapsed":153,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"6abc2d23-d247-43a5-a4ed-c0f942c750e8"},"source":["tkn = Tokenizer(spacy)\n","print(coll_repr(tkn(txt), 31))"],"execution_count":34,"outputs":[{"output_type":"stream","text":["(#302) ['xxbos','xxmaj','tenshu','is','imprisoned','and','sentenced','to','death','.','xxmaj','when','he','survives','electrocution','the','government','officials','give','him','a','choice','to','either','be','electrocute','at','a','greater','degree','or'...]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aIDyVi7ETZ1U"},"source":["Checking the default rules for fastai's text processing.\n","\n","Here is a brief summary of what each does:\n","\n","* fix_html:: Replaces special HTML characters with a readable version (IMDb reviews have quite a few of these)\n","* replace_rep:: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of times it's repeated, then the character\n","* replace_wrep:: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of times it's repeated, then the word\n","* spec_add_spaces:: Adds spaces around / and #\n","* rm_useless_spaces:: Removes all repetitions of the space character\n","* replace_all_caps:: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it\n","* replace_maj:: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it\n","* lowercase:: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)"]},{"cell_type":"code","metadata":{"id":"UK0Yi1b3TZ1V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618177109418,"user_tz":240,"elapsed":174,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"dce1d58f-bb57-41e1-d4e7-9ddb9bff1a24"},"source":["defaults.text_proc_rules"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<function fastai.text.core.fix_html>,\n"," <function fastai.text.core.replace_rep>,\n"," <function fastai.text.core.replace_wrep>,\n"," <function fastai.text.core.spec_add_spaces>,\n"," <function fastai.text.core.rm_useless_spaces>,\n"," <function fastai.text.core.replace_all_caps>,\n"," <function fastai.text.core.replace_maj>,\n"," <function fastai.text.core.lowercase>]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"G74zszs5TZ1V"},"source":["Checking out some more rules in action:"]},{"cell_type":"code","metadata":{"id":"vYibJyEiTZ1V","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1618177110832,"user_tz":240,"elapsed":278,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"1e22825e-0280-462c-81dd-d867839707a7"},"source":["coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\""]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"a9KNp0b7TZ1W"},"source":["### Subword Tokenization\n","\n","Word tokenizers use spaces to separate words/meanings; however, in a language like Chinese, there aren't any spaces to do this. Subword tokenization has two steps:\n","\n","1. Scan the corpus of documents to find sthe most commonly occurring groups of letters. These become the vocab.\n","\n","2. Tokenize the corpus using this vocab of *subword units*."]},{"cell_type":"markdown","metadata":{"id":"dLAnYcdfTZ1W"},"source":["Read first 100 files (movie reviews) into an L object."]},{"cell_type":"code","metadata":{"id":"8t-wLi3zTZ1W","executionInfo":{"status":"ok","timestamp":1618177112775,"user_tz":240,"elapsed":356,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":["txts = L(o.open().read() for o in files[:100])"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"61JxG5S1TZ1W"},"source":["Instantiate a tokenizer with vocab size `sz`. `setup` reads all documents and finds the common sequences of characters to create the vocab."]},{"cell_type":"code","metadata":{"id":"uZKrznpuTZ1X","executionInfo":{"status":"ok","timestamp":1618177113286,"user_tz":240,"elapsed":143,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":["def subword(sz):\n","    sp = SubwordTokenizer(vocab_sz=sz)\n","    sp.setup(txts)\n","    return ' '.join(first(sp([txt]))[:40])"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Av8mv7QqTZ1X"},"source":["Call our function `subword`, creating vocab of size 1000:"]},{"cell_type":"code","metadata":{"id":"MFygZ204TZ1X","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1618177115279,"user_tz":240,"elapsed":687,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"a91a28c1-7f72-486a-bd49-976e1de4d206"},"source":["subword(1000)"],"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'▁T en sh u ▁is ▁im p ris on ed ▁and ▁s ent ence d ▁to ▁de a th . ▁Whe n ▁he ▁surviv es ▁ el ect ro c ut ion ▁the ▁go ver n ment ▁of f ic'"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"j5ArU1vhTZ1X"},"source":["If we use a smaller vocab, each token represents fewer characters (tokens are simpler). Thus, it takes more tokens to represent a sentence."]},{"cell_type":"code","metadata":{"id":"7x4QExCdTZ1Y","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1618177116491,"user_tz":240,"elapsed":1117,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"38c2d598-2831-412e-8f46-c3e84ac26616"},"source":["subword(200)"],"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'▁T en sh u ▁is ▁ i m p r i s on ed ▁and ▁s ent en ce d ▁to ▁de a th . ▁W h en ▁he ▁s ur v i ve s ▁ el ect ro c'"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"lYOg7hd3TZ1Y"},"source":["If we use a larger vocab, most common English words will end up in the vocab themselves, and we will not need as many tokens to represent a sentence."]},{"cell_type":"code","metadata":{"id":"UR9pcwHnTZ1Y","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1618177116786,"user_tz":240,"elapsed":456,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"2ef3391b-cd16-434b-cb7a-5c7083ddf41d"},"source":["subword(10000)"],"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'▁T en s hu ▁is ▁im prisone d ▁and ▁s ent ence d ▁to ▁death . ▁Whe n ▁he ▁survive s ▁electrocution ▁the ▁ govern ment ▁ official s ▁give ▁him ▁a ▁cho ice ▁to ▁either ▁be ▁electrocut e ▁at'"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"OP-2ugvHTZ1Y"},"source":["Larger vocab --> fewer tokens per sentence\n","* faster training\n","* less memory\n","* less state for the model to remember\n","* requires larger embedding matrices, which require more data to learn\n","\n","Subword tokenization scales easily between character tokenization (small subword vocab) and word tokenization (large subword vocab). It can also handle every human language without needing language-specific algorithms."]},{"cell_type":"markdown","metadata":{"id":"VNtu4NDBTZ1Z"},"source":["### Numericalization with fastai\n","\n","Numericalization: mapping tokens to integers."]},{"cell_type":"markdown","metadata":{"id":"kCmaOO5yTZ1Z"},"source":["Use the word tokenizer to convert the text into tokens:"]},{"cell_type":"code","metadata":{"id":"hB6_lYI1TZ1Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618177119068,"user_tz":240,"elapsed":150,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"cdb43593-ce1b-4ea1-b6ee-4634d8f7e967"},"source":["tokens = tkn(txt)\n","print(coll_repr(tokens, 31))"],"execution_count":42,"outputs":[{"output_type":"stream","text":["(#302) ['xxbos','xxmaj','tenshu','is','imprisoned','and','sentenced','to','death','.','xxmaj','when','he','survives','electrocution','the','government','officials','give','him','a','choice','to','either','be','electrocute','at','a','greater','degree','or'...]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZNugrDtKTZ1Z"},"source":["Word-tokenize the first 200 texts:"]},{"cell_type":"code","metadata":{"id":"Q84ofEhHTZ1a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618177120175,"user_tz":240,"elapsed":584,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"920f08c5-3c98-48b4-b9e2-1c57e4c8c936"},"source":["tokens200 = txts[:200].map(tkn)\n","tokens200[0]"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#302) ['xxbos','xxmaj','tenshu','is','imprisoned','and','sentenced','to','death','.'...]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"zypkM9lGTZ1a"},"source":["Pass the list of tokens to `setup` to generate the vocab:"]},{"cell_type":"code","metadata":{"id":"lrbWMwF1TZ1b","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1618177120532,"user_tz":240,"elapsed":121,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"5723a4ee-1d3a-4501-bcf3-2827779b2cdb"},"source":["num = Numericalize()\n","num.setup(tokens200)\n","coll_repr(num.vocab, 20)"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"(#1272) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','and','a','of','to','is','in','it','i'...]\""]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"tBp_UejqTZ1b"},"source":["Special rules appear first, then each word appears once in frequency order. `Numericalize` defaults are `min_freq=3` and `max_vocab=60000`. `max_vocab=60000` causes fastai to replace all words other than the most common 60000 with the special unknown word token, xxunk. `min_freq=3` replaces any word that appears less than 3 times with xxunk. Both avoid having a large embedding matrix (which slows down training and takes up a lot of memory). However, `min_freq=3` prevents requiring data to train useful representations for rare words."]},{"cell_type":"markdown","metadata":{"id":"xY9zUmUyTZ1b"},"source":["We can call our `Numericalize` object like a function, passing it the tokens we generated from the first file. It returns a tensor of integers that represent their index in the vocab."]},{"cell_type":"code","metadata":{"id":"W_DFEsIYTZ1b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618177122930,"user_tz":240,"elapsed":140,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"0c5a3fda-a0c6-4d36-da82-45e26807225c"},"source":["nums = num(tokens)[:20]\n","nums"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorText([  2,   8,   0,  16,   0,  12,   0,  15, 376,  10,   8,  58,  38,   0,\n","          0,   9,   0,   0, 206,  87])"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"d_ZyRWBXTZ1c"},"source":["Use the indexes in `nums` to look the corresponding words up in the vocab, and concatenate them together:"]},{"cell_type":"code","metadata":{"id":"LzOSXPPTTZ1c","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1618177123579,"user_tz":240,"elapsed":144,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"e1f3568a-19b8-4d11-83ff-b8f6174d1e87"},"source":["' '.join(num.vocab[o] for o in nums)"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'xxbos xxmaj xxunk is xxunk and xxunk to death . xxmaj when he xxunk xxunk the xxunk xxunk give him'"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"gL6JX74_TZ1c"},"source":["### Putting Our Texts into Batches for a Language Model\n","\n","At the beginning of each epoch:\n","\n","1. Concatenate all the texts together into one long stream. To induce randomization, we shuffle the order of the documents (not the words inside of them!).\n","\n","2. Divide this stream into batches(e.g. if stream has 50,000 tokens and we want a batch size of 10, this will give us 10 mini-streams (batches) of 5,000 tokens. We need to preserve the order of the tokens within each mini-stream.\n","\n","`LMDataLoader` does all this for us."]},{"cell_type":"markdown","metadata":{"id":"A2q4eyftTZ1c"},"source":["Numericalize all tokens in `tokens200` (the first 200 documents)."]},{"cell_type":"code","metadata":{"id":"xsSGbBYvTZ1d","executionInfo":{"status":"ok","timestamp":1618177126031,"user_tz":240,"elapsed":385,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":["nums200 = tokens200.map(num)"],"execution_count":47,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DFLsoB0MTZ1d"},"source":["We use `LMDataLoader`, which (1) shuffles the collection of documents, (2) concatenates them into a stream of tokens, and (3) cuts the stream into a batch of fixed-size consecutive mini-streams."]},{"cell_type":"code","metadata":{"id":"J_kK0c-cTZ1d","executionInfo":{"status":"ok","timestamp":1618177126351,"user_tz":240,"elapsed":149,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":["dl = LMDataLoader(nums200)"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rHzjB8-wTZ1d"},"source":["We can grab the first batch. `x` contains 72 mini-streams, each with 64 integers (each is an index to lookup a word in the vocab)."]},{"cell_type":"code","metadata":{"id":"znlY3NvXTZ1d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618177127011,"user_tz":240,"elapsed":146,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"f6f2bc0b-4787-4691-b326-f2d55f76766b"},"source":["x,y = first(dl)\n","x.shape, y.shape"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([64, 72]), torch.Size([64, 72]))"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"QVMy9NvlTZ1e"},"source":["We examine the first row of the independent variable (`x`), which is the start of the first text:"]},{"cell_type":"code","metadata":{"id":"It5xUsYSTZ1e","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1618177127891,"user_tz":240,"elapsed":382,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"04463a37-8438-4544-83b7-7b5a78a013f4"},"source":["' '.join(num.vocab[o] for o in x[0][:20])"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'xxbos xxmaj xxunk is xxunk and xxunk to death . xxmaj when he xxunk xxunk the xxunk xxunk give him'"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"AE-dY011TZ1e"},"source":["The dependent variable is the same thing, but offset by one token:"]},{"cell_type":"code","metadata":{"id":"7LfyZafLTZ1e","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1618177128519,"user_tz":240,"elapsed":142,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"89521a84-6ccd-4691-ebb0-910fd67ed821"},"source":["' '.join(num.vocab[o] for o in y[0][:20])"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'xxmaj xxunk is xxunk and xxunk to death . xxmaj when he xxunk xxunk the xxunk xxunk give him a'"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"xZ1VsWYhTZ1e"},"source":["## Training a Text Classifier\n","\n","Below, we will fine-tune our language model (which was pretrained on Wikipedia) to the corpus of IMDb reviews, and then we can use that model to train a classifier."]},{"cell_type":"markdown","metadata":{"id":"6IjV_enxTZ1f"},"source":["### Language Model Using DataBlock\n","\n","When `TextBlock` is passed to `DataBlock`, tokenization and numericalization happen automatically. "]},{"cell_type":"code","metadata":{"id":"wPQcJSL_TZ1f","executionInfo":{"status":"ok","timestamp":1618175821339,"user_tz":240,"elapsed":141,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":["get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"loeNpoB-1b16"},"source":["If the *counter.pkl* file is not being created when you untar the `URLs.IMDB` data (you receive the `FileNotFound` error), run the below cell:"]},{"cell_type":"code","metadata":{"id":"-fSh-EyA4DPy"},"source":["import shutil\n","shutil.rmtree('') # Put the path to imdb_tok within the quotes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WSo4-SzLTZ1f","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1618177061570,"user_tz":240,"elapsed":300280,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"50f8ba12-5f09-45db-9688-44c7db4cf2ef"},"source":["path = untar_data(URLs.IMDB, force_download=True)\n","\n","dls_lm = DataBlock(\n","    blocks=TextBlock.from_folder(path, is_lm=True),\n","    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",").dataloaders(path, path=path, bs=128, seq_len=80)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["A new version of this dataset is available, downloading...\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["(#7) [Path('/root/.fastai/data/imdb_tok/train'),Path('/root/.fastai/data/imdb_tok/lengths.pkl'),Path('/root/.fastai/data/imdb_tok/tmp_lm'),Path('/root/.fastai/data/imdb_tok/counter.pkl'),Path('/root/.fastai/data/imdb_tok/test'),Path('/root/.fastai/data/imdb_tok/unsup'),Path('/root/.fastai/data/imdb_tok/tmp_clas')]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"QlNRVRieTZ1f","colab":{"base_uri":"https://localhost:8080/","height":179},"executionInfo":{"status":"ok","timestamp":1618177093843,"user_tz":240,"elapsed":1113,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"6aa8237a-d3fe-489b-e9e7-fe5357e6f77f"},"source":["dls_lm.show_batch(max_n=2)"],"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>text_</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>xxbos i watched this in xxmaj july and even with the xxmaj christmas theme , found it touching and sensitive . xxmaj it is not for someone with a reality - mind as it is full of fantasy and lovely moments that sometimes do n't make sense . xxmaj william xxmaj russ did a grand job as xxmaj hank . i have only seen him in the remake of xxmaj the xxmaj long , xxmaj hot xxmaj summer where he</td>\n","      <td>i watched this in xxmaj july and even with the xxmaj christmas theme , found it touching and sensitive . xxmaj it is not for someone with a reality - mind as it is full of fantasy and lovely moments that sometimes do n't make sense . xxmaj william xxmaj russ did a grand job as xxmaj hank . i have only seen him in the remake of xxmaj the xxmaj long , xxmaj hot xxmaj summer where he played</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of xxmaj scream but these new xxmaj saw knockoffs are beginning to make those films look like classics . xxmaj they still pander to the same demographic that those other movies were so successful at doing , but now they add a new level of degeneracy that make the twelve to fourteen year old girls they 're aimed at feel like they 're hardcore xxup and hip . \\n\\n xxmaj this movie is a load of boring crap ! xxmaj</td>\n","      <td>xxmaj scream but these new xxmaj saw knockoffs are beginning to make those films look like classics . xxmaj they still pander to the same demographic that those other movies were so successful at doing , but now they add a new level of degeneracy that make the twelve to fourteen year old girls they 're aimed at feel like they 're hardcore xxup and hip . \\n\\n xxmaj this movie is a load of boring crap ! xxmaj what</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Af0Mry-_TZ1f"},"source":["### Fine-Tuning the Language Model"]},{"cell_type":"markdown","metadata":{"id":"XwXC5Oh6WAV_"},"source":["Below explained:\n","\n","*   `AWD_LSTM`:: architecture that feeds embeddings into a recurrent neural network (RNN)\n","*   `Perplexity`:: metric that is the exponential of the loss\n","*.  `to_fp16`:: `Learner` method that converts the learner to 16-bit floating point values\n","\n"]},{"cell_type":"code","metadata":{"id":"xTSIhNwxTZ1g","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1618177231512,"user_tz":240,"elapsed":5948,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"6a918e81-dd66-4813-f266-5f7fbd355741"},"source":["learn = language_model_learner(\n","    dls_lm, AWD_LSTM, drop_mult=0.3, \n","    metrics=[accuracy, Perplexity()]).to_fp16()"],"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"XCCyTH4lXUxC"},"source":["`language_model_learner` automatically calls `freeze` when using a pretrained model, so `fit_one_cycle` will only train the embeddings (the randomly initialized weights from our IMDb vocab). The pretrained model vocab already has weights that have been fine tuned."]},{"cell_type":"code","metadata":{"id":"WZUtMbmuTZ1g","colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"status":"ok","timestamp":1618178575351,"user_tz":240,"elapsed":1342132,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"571762a7-8fe9-4bee-e036-8c7fadaafc75"},"source":["learn.fit_one_cycle(n_epoch=1, lr_max=2e-2)"],"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>perplexity</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>4.007678</td>\n","      <td>3.894840</td>\n","      <td>0.300166</td>\n","      <td>49.148201</td>\n","      <td>22:21</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Pj2bki3oTZ1g"},"source":["### Saving and Loading Models"]},{"cell_type":"markdown","metadata":{"id":"O1F9_O_AXtK3"},"source":["Creates a file in `learn.path/models/` named *1epoch.pth*:"]},{"cell_type":"code","metadata":{"id":"8KfiNAYqTZ1g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618178663686,"user_tz":240,"elapsed":1865,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"e71b677e-98c3-4b49-ac78-e8bd273673ac"},"source":["learn.save('1epoch')"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Path('/root/.fastai/data/imdb/models/1epoch.pth')"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"YV1beFplX3iU"},"source":["Load the model after creating the `Learner` the same way, or resuming training later:"]},{"cell_type":"code","metadata":{"id":"d1EPmn6ITZ1g","executionInfo":{"status":"ok","timestamp":1618178666760,"user_tz":240,"elapsed":429,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":["learn = learn.load('1epoch')"],"execution_count":55,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pmjmCnZnYG42"},"source":["After the intial training has finished, we can continue fine-tuning the model after unfreezing:"]},{"cell_type":"code","metadata":{"id":"gV2rfGMOTZ1g","colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"status":"ok","timestamp":1618180088417,"user_tz":240,"elapsed":1419875,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"6d66c245-4c69-431b-e823-9ef079869097"},"source":["learn.unfreeze()\n","learn.fit_one_cycle(1, 2e-3)"],"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>perplexity</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>3.704372</td>\n","      <td>3.666091</td>\n","      <td>0.327556</td>\n","      <td>39.098770</td>\n","      <td>23:39</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"9VDyiA9HYQ-F"},"source":["The below line saves the model *except* for the final layer (which converts activations to probabilities of picking each token in the vocab). The model not including the final layer is called the **encoder**."]},{"cell_type":"code","metadata":{"id":"kQptDdtpTZ1h","executionInfo":{"status":"ok","timestamp":1618180091323,"user_tz":240,"elapsed":660,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":["learn.save_encoder('finetuned')"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LIkaofRBDMQc","executionInfo":{"status":"ok","timestamp":1618180168825,"user_tz":240,"elapsed":154,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"c8e04e39-1958-4639-d0f1-7ee1aecdb87e"},"source":["Path(path/'models').ls()"],"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#2) [Path('/root/.fastai/data/imdb/models/finetuned.pth'),Path('/root/.fastai/data/imdb/models/1epoch.pth')]"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"8oLDP7_lTZ1h"},"source":["### Text Generation"]},{"cell_type":"markdown","metadata":{"id":"5DSPsgsxZFNa"},"source":["We can use our model to generate random reviews. In `preds`, we store an array of sentences which the model generates."]},{"cell_type":"code","metadata":{"id":"BXJIeXuxTZ1h","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1618180219800,"user_tz":240,"elapsed":2169,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"6b945cb1-1fe9-4c9f-9802-aeadc63b39d1"},"source":["TEXT = \"I liked this movie because\"\n","N_WORDS = 40\n","N_SENTENCES = 2\n","preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n","          for _ in range(N_SENTENCES)]"],"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"0zObUK9KZYy6"},"source":["There is some randomness in which word is chosen (based on the probabilities returned by the model) so that we don't get exactly the same review:"]},{"cell_type":"code","metadata":{"id":"twIvRka-TZ1h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618180220613,"user_tz":240,"elapsed":147,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"ce7268b6-9530-4981-c552-1e06eeb8c8dd"},"source":["print(\"\\n\".join(preds))"],"execution_count":63,"outputs":[{"output_type":"stream","text":["i liked this movie because of the Australian English accents . It was a Aussie movie . \n","\n"," It was somewhat fun to see Australian director Bryan Kirkwood at work in the Aussie film industry as\n","i liked this movie because the film is really funny . The film was excellent , and i just love it . This movie was perfect . My Rating : 10 / 10 This film is the perfect example\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Nk2uuHhfTZ1h"},"source":["### Creating the Classifier DataLoaders"]},{"cell_type":"markdown","metadata":{"id":"gyPRyuBPZ8iH"},"source":["Here we create an NLP classifier data block. We pass our language model's `vocab` into this `DataBlock`, and we use `CategoryBlock` to create the classifier. We pass the `vocab` in because we want to use the same correspondence of token to index. Otherwise, the embeddings learned in our fine-tuned language model won't make sense to this model.\n","\n","We do not specify `is_lm=True`, since we have regular labeled data, not using the next tokens as labels."]},{"cell_type":"code","metadata":{"id":"8PhhDoQ5TZ1h","executionInfo":{"status":"ok","timestamp":1618180229506,"user_tz":240,"elapsed":5417,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":["dls_clas = DataBlock(\n","    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n","    get_y = parent_label,\n","    get_items=partial(get_text_files, folders=['train', 'test']),\n","    splitter=GrandparentSplitter(valid_name='test')\n",").dataloaders(path, path=path, bs=128, seq_len=72)"],"execution_count":64,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HwO8HyANbrGN"},"source":["We show the batch, with the independent variable (text) and dependent variable (sentiment):"]},{"cell_type":"code","metadata":{"id":"HZQRWC8sTZ1i","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"ok","timestamp":1618180232509,"user_tz":240,"elapsed":400,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"bdd9c135-a19c-4fba-ad9b-9a416684c2a1"},"source":["dls_clas.show_batch(max_n=3)"],"execution_count":65,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>xxbos * ! ! - xxup spoilers - ! ! * \\n\\n xxmaj before i begin this , let me say that i have had both the advantages of seeing this movie on the big screen and of having seen the \" authorized xxmaj version \" of this movie , remade by xxmaj stephen xxmaj king , himself , in 1997 . \\n\\n xxmaj both advantages made me appreciate this version of \" the xxmaj shining , \" all the more . \\n\\n xxmaj also , let me say that xxmaj i 've read xxmaj mr . xxmaj king 's book , \" the xxmaj shining \" on many occasions over the years , and while i love the book and am a huge fan of his work , xxmaj stanley xxmaj kubrick 's retelling of this story is far more compelling … and xxup scary . \\n\\n xxmaj kubrick</td>\n","      <td>pos</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>xxbos xxrep 3 * xxmaj warning - this review contains \" plot spoilers , \" though nothing could \" spoil \" this movie any more than it already is . xxmaj it really xxup is that bad . xxrep 3 * \\n\\n xxmaj before i begin , xxmaj i 'd like to let everyone know that this definitely is one of those so - incredibly - bad - that - you - fall - over - laughing movies . xxmaj if you 're in a lighthearted mood and need a very hearty laugh , this is the movie for you . xxmaj now without further ado , my review : \\n\\n xxmaj this movie was found in a bargain bin at wal - mart . xxmaj that should be the first clue as to how good of a movie it is . xxmaj secondly , it stars the lame action</td>\n","      <td>neg</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"EheVugWGc6IO"},"source":["Create a mini-batch with the first 10 documents, numericalizing all of them:"]},{"cell_type":"code","metadata":{"id":"qtqpepmrTZ1i","executionInfo":{"status":"ok","timestamp":1618180256947,"user_tz":240,"elapsed":145,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":["nums_samp = tokens200[:10].map(num)"],"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"grQqNv2hdEdz"},"source":["Show the length of each tokenized and numericalized document. We see that the documents are all different lengths. With images, we were able to correct this by cropping, padding, and/or squishing to make the inputs the same size.\n","\n","We use padding to augment the text data in a similar way. We first batch together texts that are roughly the same lengths. We then expand the shortest texts to make them the same size as the largest text in the batch, using a special padding token that is ignored by the model."]},{"cell_type":"code","metadata":{"id":"tfBRS22vdDo3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618180259631,"user_tz":240,"elapsed":369,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"f075368f-f05b-4a9f-d6da-0667859f9692"},"source":["nums_samp.map(len)"],"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(#10) [302,225,184,279,157,145,179,130,423,492]"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"markdown","metadata":{"id":"8InI9F1pdN1C"},"source":["The `TextBlock` class does the text batching/padding for us when `is_lm=False`."]},{"cell_type":"code","metadata":{"id":"e3jpzJTUTZ1i","executionInfo":{"status":"ok","timestamp":1618180262417,"user_tz":240,"elapsed":1674,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":["learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n","                                metrics=accuracy).to_fp16()"],"execution_count":69,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DyzTM8J3eLtB"},"source":["We will now load the encoder from the fine-tuned language model:"]},{"cell_type":"code","metadata":{"id":"lixB1w2sTZ1i","executionInfo":{"status":"ok","timestamp":1618180264401,"user_tz":240,"elapsed":347,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":["learn = learn.load_encoder('finetuned')"],"execution_count":70,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jKvtnuGCTZ1i"},"source":["### Fine-Tuning the Classifier"]},{"cell_type":"code","metadata":{"id":"Oi2AjTpGTZ1i","colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"status":"ok","timestamp":1618180363190,"user_tz":240,"elapsed":74082,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"a4f2845f-49be-4501-be27-0af5029f2cad"},"source":["learn.fit_one_cycle(1, 2e-2)"],"execution_count":71,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.288900</td>\n","      <td>0.225328</td>\n","      <td>0.908480</td>\n","      <td>01:13</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"mT3kMtr2TZ1j","colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"status":"ok","timestamp":1618180467856,"user_tz":240,"elapsed":79133,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"63c6fefd-8fbe-47cb-ca0d-5b11ee268e94"},"source":["learn.freeze_to(-2)\n","learn.fit_one_cycle( 1, slice(1e-2/2.6**4, 1e-2) )"],"execution_count":72,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.248256</td>\n","      <td>0.190440</td>\n","      <td>0.926120</td>\n","      <td>01:18</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"hkTOz9n2TZ1j","colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"status":"ok","timestamp":1618180568876,"user_tz":240,"elapsed":99672,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"4fd0ad60-041d-4f4d-f3bb-13b2d57e06a2"},"source":["learn.freeze_to(-3)\n","learn.fit_one_cycle( 1, slice(5e-3/(2.6**4), 5e-3) )"],"execution_count":73,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.206388</td>\n","      <td>0.170632</td>\n","      <td>0.935720</td>\n","      <td>01:39</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"XnAuiObWTZ1j","colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"status":"ok","timestamp":1618180709115,"user_tz":240,"elapsed":120591,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}},"outputId":"1df96f50-1b76-49d7-fd4d-f22746c8da12"},"source":["learn.unfreeze()\n","learn.fit_one_cycle( 1, slice(1e-3/2.6**4, 1e-3) )"],"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.165760</td>\n","      <td>0.166050</td>\n","      <td>0.938200</td>\n","      <td>02:00</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"nxX0FTUcTZ1j"},"source":["## Disinformation and Language Models"]},{"cell_type":"markdown","metadata":{"id":"DCpvIocDfLcS"},"source":["The vast improvement of language models has lead to a rise in disinformation from AI-generated text online. Language models have learned English well enough to the point where posts written by AI are almost indistinguishable from a normal human's writing. Auto-generated profiles/bots abound on social media, and it is a challenge to see whether we can develop models good enough to detect whether something has been created by AI."]},{"cell_type":"markdown","metadata":{"id":"T8ITKulSTZ1j"},"source":["## Conclusion"]},{"cell_type":"markdown","metadata":{"id":"Y7TrdxZXf7d-"},"source":["Two types of models in this chapter:\n","\n","* Language models that can generate text\n","* Classifier that determines if a review is positive or negative\n","\n","To build the classifier, we used:\n","\n","* Pretrained language model\n","* Fine-tuned the model with the IMDb data\n","* Used the model's body (the encoder) with a new head to do classification\n"]},{"cell_type":"markdown","metadata":{"id":"4oIPoI4WTZ1j"},"source":["## Questionnaire"]},{"cell_type":"markdown","metadata":{"id":"agM8b-eCTZ1j"},"source":["1. What is \"self-supervised learning\"?\n","1. What is a \"language model\"?\n","1. Why is a language model considered self-supervised?\n","1. What are self-supervised models usually used for?\n","1. Why do we fine-tune language models?\n","1. What are the three steps to create a state-of-the-art text classifier?\n","1. How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\n","1. What are the three steps to prepare your data for a language model?\n","1. What is \"tokenization\"? Why do we need it?\n","1. Name three different approaches to tokenization.\n","1. What is `xxbos`?\n","1. List four rules that fastai applies to text during tokenization.\n","1. Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?\n","1. What is \"numericalization\"?\n","1. Why might there be words that are replaced with the \"unknown word\" token?\n","1. With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book's website.)\n","1. Why do we need padding for text classification? Why don't we need it for language modeling?\n","1. What does an embedding matrix for NLP contain? What is its shape?\n","1. What is \"perplexity\"?\n","1. Why do we have to pass the vocabulary of the language model to the classifier data block?\n","1. What is \"gradual unfreezing\"?\n","1. Why is text generation always likely to be ahead of automatic identification of machine-generated texts?"]},{"cell_type":"markdown","metadata":{"id":"FkRkKNR2TZ1k"},"source":["### Further Research"]},{"cell_type":"markdown","metadata":{"id":"Wx4m5H3vTZ1k"},"source":["1. See what you can learn about language models and disinformation. What are the best language models today? Take a look at some of their outputs. Do you find them convincing? How could a bad actor best use such a model to create conflict and uncertainty?\n","1. Given the limitation that models are unlikely to be able to consistently recognize machine-generated texts, what other approaches may be needed to handle large-scale disinformation campaigns that leverage deep learning?"]},{"cell_type":"code","metadata":{"id":"LmTFPOSITZ1k","executionInfo":{"status":"aborted","timestamp":1618175100389,"user_tz":240,"elapsed":39703,"user":{"displayName":"Zachary Chin","photoUrl":"","userId":"00634451652335778717"}}},"source":[""],"execution_count":null,"outputs":[]}]}